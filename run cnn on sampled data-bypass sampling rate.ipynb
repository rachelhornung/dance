{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras as K\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    pass\n",
    "\n",
    "config = Config()\n",
    "config.log_dir = 'tf_logs/split_cnn/'+str(time.strftime('%Y-%m%d %H:%M:%S'))\n",
    "config.batch_size = 50\n",
    "config.epochs = 400\n",
    "config.kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "config.pool_size = 2 # we will use 2x2 pooling throughout\n",
    "config.drop_rate =0.25\n",
    "config.conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
    "config.conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n",
    "\n",
    "config.pool_strides = 2\n",
    "config.conv_strides = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dance.audiosamples import RectifiedAudioSamples as AudioSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(\"train_data.pkl\")\n",
    "val_data = pd.read_pickle(\"val_data.pkl\")\n",
    "test_data = pd.read_pickle(\"test_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_samples(train_data, val_data, test_data, sampler):\n",
    "    try:\n",
    "        del(train_samples)\n",
    "        del(val_samples)\n",
    "        del(test_samples)\n",
    "    except:\n",
    "        pass\n",
    "    train_samples = sampler(train_data, subsample_rate=2)\n",
    "    scaler = train_samples.norm()\n",
    "    encoder = train_samples.encode()\n",
    "    val_samples=sampler(val_data, subsample_rate=2)\n",
    "    val_samples.norm(scaler)\n",
    "    val_samples.encode(encoder)\n",
    "    test_samples = sampler(test_data, subsample_rate=2)\n",
    "    test_samples.norm(scaler)\n",
    "    test_samples.encode(encoder)\n",
    "    return train_samples, val_samples, test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, val, test = generate_samples(train_data, val_data, test_data, AudioSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "#model = K.models.Sequential()\n",
    "rate_input = K.layers.Input(shape=(1,))\n",
    "wave_input = K.layers.Input(shape=((train.X.shape[1]-1,1,)))\n",
    "                    \n",
    "z = K.layers.Conv1D(config.conv_depth_1, config.kernel_size, name='w_con_1/conv')(wave_input)\n",
    "z = K.layers.Activation('softplus', name='w_con_1/act')(z)\n",
    "z = K.layers.MaxPooling1D(config.pool_size, config.pool_strides, name='w_con_1/pool')(z)\n",
    "#z = K.layers.Dropout(config.drop_rate, name='w_con_1/drop')(z)\n",
    "\n",
    "z = K.layers.Conv1D(config.conv_depth_2, config.kernel_size, activation='softplus', name='w_con_2/conv')(z)\n",
    "# z = K.layers.Activation('softmax', name='w_con_2/act')(z)\n",
    "z = K.layers.MaxPooling1D(config.pool_size, config.pool_strides, name='w_con_2/pool')(z)\n",
    "#z = K.layers.BatchNormalization(name='w_con_2/batch_norm')(z)\n",
    "\n",
    "z = K.layers.Flatten(name='flatten')(z)\n",
    "x = K.layers.concatenate([rate_input, z], axis=1, name='concat')\n",
    "\n",
    "x = K.layers.Dense(100, activation='softplus', kernel_regularizer=K.regularizers.l2(), name='dense')(x)\n",
    "x = K.layers.Dense(train.Y.shape[-1], activation='softmax', kernel_regularizer=K.regularizers.l2(), name='loss')(x)\n",
    "\n",
    "\n",
    "model = K.models.Model((rate_input, wave_input), x, name='split_cnn')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model._make_train_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39648 samples, validate on 10166 samples\n",
      "INFO:tensorflow:Summary name w_con_1/conv/kernel:0 is illegal; using w_con_1/conv/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name w_con_1/conv/bias:0 is illegal; using w_con_1/conv/bias_0 instead.\n",
      "INFO:tensorflow:Summary name w_con_2/conv/kernel:0 is illegal; using w_con_2/conv/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name w_con_2/conv/bias:0 is illegal; using w_con_2/conv/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0 is illegal; using dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0 is illegal; using dense/bias_0 instead.\n",
      "INFO:tensorflow:Summary name loss/kernel:0 is illegal; using loss/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name loss/bias:0 is illegal; using loss/bias_0 instead.\n",
      "Epoch 1/400\n",
      "39648/39648 [==============================] - 606s - loss: 8.3293 - acc: 0.2352 - val_loss: 1.9101 - val_acc: 0.2392\n",
      "Epoch 2/400\n",
      "39648/39648 [==============================] - 441s - loss: 1.8115 - acc: 0.2992 - val_loss: 1.9272 - val_acc: 0.2476\n",
      "Epoch 3/400\n",
      "39648/39648 [==============================] - 442s - loss: 1.7959 - acc: 0.3024 - val_loss: 2.0090 - val_acc: 0.2547\n",
      "Epoch 4/400\n",
      "39648/39648 [==============================] - 436s - loss: 1.7847 - acc: 0.3035 - val_loss: 1.9996 - val_acc: 0.2585\n",
      "Epoch 5/400\n",
      "39648/39648 [==============================] - 454s - loss: 1.7761 - acc: 0.3066 - val_loss: 1.9691 - val_acc: 0.2825\n",
      "Epoch 6/400\n",
      "39648/39648 [==============================] - 434s - loss: 1.7712 - acc: 0.3095 - val_loss: 1.8815 - val_acc: 0.2814\n",
      "Epoch 7/400\n",
      "39648/39648 [==============================] - 510s - loss: 1.7648 - acc: 0.3090 - val_loss: 1.8656 - val_acc: 0.2868\n",
      "Epoch 8/400\n",
      "39648/39648 [==============================] - 433s - loss: 1.7618 - acc: 0.3096 - val_loss: 1.9278 - val_acc: 0.2893\n",
      "Epoch 9/400\n",
      "39648/39648 [==============================] - 433s - loss: 1.7597 - acc: 0.3103 - val_loss: 1.9755 - val_acc: 0.2882\n",
      "Epoch 10/400\n",
      "39648/39648 [==============================] - 479s - loss: 1.7562 - acc: 0.3127 - val_loss: 1.8796 - val_acc: 0.3045\n",
      "Epoch 11/400\n",
      "39648/39648 [==============================] - 647s - loss: 1.7516 - acc: 0.3132 - val_loss: 1.9028 - val_acc: 0.3003\n",
      "Epoch 12/400\n",
      "39648/39648 [==============================] - 509s - loss: 1.7492 - acc: 0.3121 - val_loss: 1.9089 - val_acc: 0.2837\n",
      "Epoch 13/400\n",
      "39648/39648 [==============================] - 532s - loss: 1.7490 - acc: 0.3108 - val_loss: 1.8866 - val_acc: 0.2965\n"
     ]
    }
   ],
   "source": [
    "tbCallback = K.callbacks.TensorBoard(config.log_dir, batch_size=config.batch_size, histogram_freq=10)\n",
    "patience = K.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit([train.X[:,0], train.get_as_timeseries()[:,1:]], train.Y, epochs=config.epochs, \n",
    "                    batch_size=config.batch_size, \n",
    "                    validation_data=([val.X[:,0], val.get_as_timeseries()[:,1:]], val.Y), \n",
    "                    callbacks=[tbCallback, patience],\n",
    "                    verbose=True,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18384/18384 [==============================] - 67s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1398003557850944, 0.1656875543951262]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([test.X[:, 0], test.get_as_timeseries()[:, 1:]], y=test.Y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
